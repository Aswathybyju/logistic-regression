{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9cc488-3836-4d52-84ce-eed149722ccb",
   "metadata": {},
   "source": [
    "### Optimise alpha (learning rate) using validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea9a164-488b-4694-951a-78c89264ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef375a62-b2c1-410c-b809-1b213ec71a7a",
   "metadata": {},
   "source": [
    "#### Preparing validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82e4d97-7e0a-4dd9-a9c5-517381b6bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val shape: (27, 3)\n",
      "y_val shape: (27, 1)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files for the testing data\n",
    "X_val = pd.read_csv('data/X_val.csv').values  # Convert the DataFrame to a NumPy array\n",
    "y_val = pd.read_csv('data/y_val.csv').values  # Convert the DataFrame to a NumPy array\n",
    "\n",
    "# Print the shape of the arrays to verify\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8975a-e9bb-488b-a1b1-7a5a19d4e94d",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4995ab59-763b-4a78-9b78-3b143e305a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler():\n",
    "    def fit(self, X):\n",
    "        self.mean = X.mean(axis=0)\n",
    "        self.std = X.std(axis=0)\n",
    "\n",
    "        # the standard deviation can be 0 in certain cases,\n",
    "        #  which provokes 'devision-by-zero' errors; we can\n",
    "        #  avoid this by adding a small amount if std==0\n",
    "        self.std[self.std == 0] = 0.00001\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        return X_scaled * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551d7511-0be6-450a-a3a4-7ba1dcec5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_val = StandardScaler()\n",
    "scaler_val.fit(X_val)\n",
    "\n",
    "X_val_scaled = scaler_val.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6befba34-70ea-4888-b2a4-31dbe46793ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_val = X_val.shape[0]\n",
    "X_val_with_bias = np.c_[np.ones(n_samples_val), X_val_scaled]  # Add bias term as a column of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e66fa074-46f7-48aa-bc6b-87b73311ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of classes\n",
    "n_classes = np.max(y_val)\n",
    "\n",
    "# Create an empty one-hot encoded array\n",
    "y_one_hot_val = np.zeros((y_val.size, n_classes))\n",
    "\n",
    "# Populate the one-hot array\n",
    "y_one_hot_val[np.arange(y_val.size), y_val.flatten() - 1] = 1  # Flatten y_val to 1D and subtract 1 for zero-indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515a05b-48dd-4a69-a288-b066a49c2b00",
   "metadata": {},
   "source": [
    "##### Intialise Theta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d6e64ae-4a4a-41e8-a107-f82570fbba2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27013521,  0.80223834,  0.33135093],\n",
       "       [-0.63340223,  0.55902406,  0.3065858 ],\n",
       "       [ 0.12444586,  0.92766619, -0.7410244 ],\n",
       "       [ 0.74065042,  0.45738951,  0.03656996]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import uniform\n",
    "\n",
    "# Number of features (1 bias term + 3 features)\n",
    "n_features = 4\n",
    "\n",
    "# Initialize thetas: Random weights for each feature and class\n",
    "#initial_thetas = np.array([uniform(-1, 1) for _ in range(n_features * n_classes)]).reshape(n_features, n_classes)\n",
    "\n",
    "# Initialize thetas with random values between -1 and 1\n",
    "initial_thetas = np.random.uniform(-1, 1, size=(n_features, n_classes))  # Shape: (n_features, n_classes)\n",
    "\n",
    "initial_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f91c1-9c0d-46f5-b6fd-c4cbee230c2f",
   "metadata": {},
   "source": [
    "#### Preparing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0e667ab-2d14-4a3f-aa06-0daeee508447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (126, 3)\n",
      "y_train shape: (126, 1)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files for the training data\n",
    "X_train = pd.read_csv('data/X_train.csv').values  # Convert the DataFrame to a NumPy array\n",
    "y_train = pd.read_csv('data/y_train.csv').values  # Convert the DataFrame to a NumPy array\n",
    "\n",
    "# Print the shape of the arrays to verify\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff3135fc-ae19-4b82-b100-09d6e6da55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_train = StandardScaler()\n",
    "scaler_train.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler_train.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9602550-f9f4-4125-8cf3-cf8585dd964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_train = X_train.shape[0]\n",
    "# Add a bias term (column of ones) to the scaled training dataset to account for the intercept term in logistic regression (theta_bias)\n",
    "X_train_with_bias = np.c_[np.ones(n_samples_train), X_train_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8158cb59-8e18-4b70-9f71-a0cb462c3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty one-hot encoded array\n",
    "y_one_hot_train = np.zeros((y_train.size, n_classes))\n",
    "\n",
    "# Populate the one-hot array\n",
    "y_one_hot_train[np.arange(y_train.size), y_train.flatten() - 1] = 1  # Flatten y_train to 1D and subtract 1 for zero-indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe05c3-ae6d-46ec-8edb-803760aae12e",
   "metadata": {},
   "source": [
    "#### Implementing Gradient descent for obtaining optimized theta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9919a5b1-aac8-467d-a8cb-3dbb7524be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(data, thetas):\n",
    "    \"\"\"\n",
    "    Perform linear regression to compute raw scores (logits).\n",
    "    data: np.ndarray, shape (n_samples, n_features) - Input feature data.\n",
    "    thetas: np.ndarray, shape (n_features, n_classes) - Model parameters.\n",
    "    \n",
    "    Returns:\n",
    "    logits: np.ndarray, shape (n_samples, n_classes) - Raw logits for each class.\n",
    "    \"\"\"\n",
    "    return data @ thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4eeb06d9-aa08-4594-8c8f-1e7ca5a1bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax probabilities for multiclass classification.\n",
    "    z: np.ndarray, shape (n_samples, n_classes) - Raw linear logits for each class.\n",
    "    \n",
    "    Returns:\n",
    "    probabilities: np.ndarray, shape (n_samples, n_classes) - Normalized probabilities.\n",
    "    \"\"\"\n",
    "    # Subtract max value in each row for numerical stability\n",
    "    z_exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    probabilities = z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06f07b9e-4952-4eb7-8857-6ee94d04af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(data, thetas):\n",
    "    \"\"\"\n",
    "    Perform multiclass classification using softmax.\n",
    "    data: np.ndarray, shape (n_samples, n_features) - Input feature data.\n",
    "    thetas: np.ndarray, shape (n_features, n_classes) - Model parameters.\n",
    "    \n",
    "    Returns:\n",
    "    probabilities: np.ndarray, shape (n_samples, n_classes) - Probabilities for each class.\n",
    "    \"\"\"\n",
    "    z = linear_regression(data, thetas)  # Compute raw logits\n",
    "    h = softmax(z)                       # Apply softmax to logits\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84343064-6116-44c4-af97-ca7ce8964bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for multiclass classification.\n",
    "\n",
    "    Parameters:\n",
    "    y_true: np.ndarray, shape (n_samples, n_classes) - One-hot encoded ground truth labels.\n",
    "    y_pred: np.ndarray, shape (n_samples, n_classes) - Predicted probabilities (from softmax).\n",
    "\n",
    "    Returns:\n",
    "    loss: float - Cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Avoid log(0) issues by adding a small epsilon\n",
    "    epsilon = 1e-12\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Compute cross-entropy loss\n",
    "    loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b79f3fe1-f01a-4871-ba48-fe08ab9e99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, y_true, original_thetas, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization for multiclass logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    data: np.ndarray, shape (n_samples, n_features) - Feature matrix with bias term.\n",
    "    y_true: np.ndarray, shape (n_samples, n_classes) - One-hot encoded ground truth labels.\n",
    "    original_thetas: np.ndarray, shape (n_features, n_classes) - Initial model parameters.\n",
    "    alpha: float - Learning rate for gradient descent.\n",
    "    iterations: int - Number of iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "    optimized_thetas: np.ndarray, shape (n_features, n_classes) - Optimized model parameters.\n",
    "    error_history: list - Cross-entropy error at each iteration.\n",
    "    \"\"\"\n",
    "    optimized_thetas = original_thetas.copy()\n",
    "    n_samples = data.shape[0]\n",
    "    error_history = np.zeros(iterations)\n",
    "    accuracy_history = np.zeros(iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Compute predictions using logistic regression (softmax applied)\n",
    "        logits = np.dot(data, optimized_thetas)  # Shape: (n_samples, n_classes)\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Numerical stability\n",
    "        y_pred = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)      # Shape: (n_samples, n_classes)\n",
    "\n",
    "        # Compute cross-entropy loss and record it\n",
    "        error = -np.sum(y_true * np.log(y_pred + 1e-12)) / n_samples  # Cross-entropy loss\n",
    "        error_history[i] = error\n",
    "\n",
    "        # Compute accuracy\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)  # Predicted class labels\n",
    "        y_true_labels = np.argmax(y_true, axis=1)  # True class labels\n",
    "        accuracy_ = round((y_pred_labels == y_true_labels).mean() * 100, 2)\n",
    "        accuracy_history[i] = accuracy_\n",
    "        \n",
    "        # Compute gradient of cross-entropy loss\n",
    "        gradient = np.dot(data.T, (y_pred - y_true)) / n_samples  # Shape: (n_features, n_classes)\n",
    "\n",
    "        # Update parameters using gradient descent\n",
    "        optimized_thetas -= alpha * gradient\n",
    "\n",
    "    return optimized_thetas, error_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a463a3cf-5d31-4366-85c5-238ca3a119ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent\n",
    "alpha = 0.01        # Learning rate\n",
    "iterations = 300  # Number of iterations\n",
    "trained_thetas, error_history, accuracy_history = gradient_descent(X_train_with_bias, y_one_hot_train, initial_thetas, alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13b440-21cd-402f-8397-02244c1bf3fe",
   "metadata": {},
   "source": [
    "#### Optimize Alpha using Train dataset, Validation set and Optimized theta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2255ded1-38a8-4135-9afa-ff37bce7d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_alpha(train_data, train_labels, val_data, val_labels, thetas):\n",
    "    \"\"\"\n",
    "    Tune the learning rate alpha using a validation dataset.\n",
    "\n",
    "    Parameters:\n",
    "    train_data: np.ndarray, shape (n_train_samples, n_features + 1) - Training feature matrix with bias term.\n",
    "    train_labels: np.ndarray, shape (n_train_samples, n_classes) - One-hot encoded training labels.\n",
    "    val_data: np.ndarray, shape (n_val_samples, n_features + 1) - Validation feature matrix with bias term.\n",
    "    val_labels: np.ndarray, shape (n_val_samples, n_classes) - One-hot encoded validation labels.\n",
    "    thetas: np.ndarray, shape (n_features + 1, n_classes) - Initial model parameters.\n",
    "    alphas: list - List of learning rates to try.\n",
    "    iterations: int - Number of iterations for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    best_alpha: float - The learning rate that minimizes the validation error.\n",
    "    validation_errors: dict - Validation errors for each alpha.\n",
    "    \"\"\"\n",
    "    alphas = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "    \n",
    "    validation_errors = {}\n",
    "    iterations = 100\n",
    "    for alpha in alphas:\n",
    "        # Train the model on the training set\n",
    "        optimized_thetas, error, accuracy = gradient_descent(train_data, train_labels, thetas, alpha, iterations)\n",
    "\n",
    "        # Compute predictions on the validation set\n",
    "        #logits = np.dot(val_data, optimized_thetas)\n",
    "        #exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Numerical stability\n",
    "        #y_pred = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        y_pred = logistic_regression(val_data, optimized_thetas)\n",
    "\n",
    "        # Compute cross-entropy loss on the validation set\n",
    "        #val_error = -np.sum(val_labels * np.log(y_pred + 1e-12)) / val_data.shape[0]\n",
    "        val_error = cross_entropy_loss(val_labels, y_pred)\n",
    "        validation_errors[alpha] = val_error\n",
    "\n",
    "    # Select the alpha with the minimum validation error\n",
    "    best_alpha = min(validation_errors, key=validation_errors.get)\n",
    "\n",
    "    return best_alpha, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33e43908-744f-4ac5-83f9-857246f1bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_alpha, validation_errors = tune_alpha(X_train_with_bias, y_one_hot_train, X_val_with_bias, y_one_hot_val, trained_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2c8d133-da12-421c-a27e-6a64e17dbc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f1ac14a-3727-464b-b9c0-e26d1dfbdea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1e-05: 0.4099772192785057,\n",
       " 0.0001: 0.4093095911643733,\n",
       " 0.001: 0.4028528317820569,\n",
       " 0.01: 0.35473424634456496,\n",
       " 0.1: 0.2396892330209989}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d9401-44ca-4942-b311-8ff9fcdd3d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
